---
title: "Regression models in R"
author: "Wim Bernasco"
format: html
editor: visual
---

## Packages for this session

It is usually a good idea to load all packages at the top of the script. In this script, we use the `tidyverse` suite of packages, `broom` and `faux`.

```{r}
library(tidyverse)
library(broom)
library(faux)
```



## Create synthetic data

To start rightaway, I will first create a synthetic dataset, i.e., fictive data that I do not need to understand or clean up front.

When your script uses random number generators, to enhance its reproducibility you are advised to make sure the random number generator has a fixed starting point. This is called "setting the seed". I virtually always use number 123456789 because I was once advised to do so and never changed the habit.

```{r}
set.seed(123456789)
```

Using the `rnorm_multi` function from Lisa DeBruine's excellent `faux` package, I create a dataset containing 200 observations and three variables that are correlated .50 with each other. 

```{r}
my_data <- 
  rnorm_multi(
    n = 200, vars = 3, r = .50
    )
```

Let me take a quick snapshot of what the data look like. 

```{r}
my_data |> 
  head(n =10)
```

I would also like to verify that the three variables in this magically created dataset do really correlate approximately .50 with each other. Using the `print` function I limit the output to 2 decimal digits.

```{r}
my_data |>
  cor() |>
  print(digits = 2)
```

## My first ordinary least squares regression

Let's assume that variable `X1` is a linear function of `X2` and `X3`, and that we want to estimate this function using ordinary least squares regression. This implies that `X1` is the dependent variable and `X2` and `X3` are the independent variables. Estimating this regression model is also referred to as "regressing `X1` on `X2` and `X3`".

The function that estimates the ordinary least squares regression model in R is called `lm` (an acronym of linear model).

All regression functions in R have at least two parameters that must be specified. One is the **data** used for estimation, the other is the equation to be estimated (which is called a **formula** in R). The formula must contain only variables included in the data.

Let me first do it and then explain.

```{r}
lm(formula = X1 ~ X2 + X3,
   data    = my_data)
```

I called the `lm` function with two arguments. The first is the `formula` argument. A formula is an equation with the right-hand side separated from the left-hand side with a tilde symbol ("~"). In regression contexts, the left-hand side contains the dependent variable, the right-hand side the independent variables. Thus, the equation `X1 ~ X2 + X3` says "estimate a linear regression of X1 on X2 and X3". Note that the regression coefficients to be estimated are implicit. Thus, although the complete regression equation is 
$$ X_{1} = \alpha + \beta_{2} \times X_{2} +
                    \beta_{3} \times X_{3}$$
                    
I do not need to specify the $\alpha$ and $\beta$ coefficients.

By default, the `lm` function will print just two things, namely (1) an echo of the data and formula arguments, and (2) the estimated values of the unstandardized coefficients, which here are the constant term and the coefficients of `X2` and `X3`. However, as we will see in a minute, a lot of additional information is being generated when you estimate a regression model.

## Storing and accessing the results

To make sure that my regression results are reproducible, I should **never** estimate a model and manually copy and paste the results. This is very error-prone and not reproducible. 

Therefore, whenever I estimate a regression model I should save the output into an object. Here is how I store the regression output in `my_model`.

```{r}
my_model <- 
  lm(formula = X1 ~ X2 + X3,
     data    = my_data)

my_model
```

Although we haven't seen it, a lot of information is contained in `my_model`. Some of it can be accessed using the `summary` function:

```{r}
my_model |>
  summary()
```

